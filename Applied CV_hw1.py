# -*- coding: utf-8 -*-
"""ACV_hw1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18Dh6bHlSv46s3MFizlYvz1RHle_e5ezA

# 1. Build a facial recognition model using the LFW Dataset (5 points)

Use the built-in dataset from scikit-learn, here:
https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_lfw_people.html

Let's build out a pipeline to inspect, train, and evaluate a computer vision model.  We will compare the following 3 algorithms and look at their performance:


*   **Linear Regression**: no hyper-parameters to explore here
*   **Random Forest**: explore 4 different variants between very shallow (5) and deep (10) trees as well as less (10) and more (150) total trees.
*   **SVM**: use a radial-basis kernel and explore at least 3 different hyper-parameter settings of your choosing

This will result in 8 different model-types to train.  In addition, we will explore with and without normalization of the data.  Here, normalization means ensuring that your data fits a 0-mean/1-stdev Gaussian distribution.  So in this way, we will total 16 different experiments (8 model-types, each with and without normalization of the input data).

You must split the dataset into a train and test split.  You may randomly split the data for this, but use 25% to set aside for testing.  **Make sure the comparisons amongst the model-types are "fair".**  

*The input features* to each of these models will be the pixel values themselves.  Recall from class how we do this with 2D images (hint: they need to be 1D for the estimators to make sense of them).  You may reduce the dimensionality of the features, but you must empirically support your choices.

*To compare the models*, b/c this is a multi-class classification problem, we will use classification accuracy as the metric.  For each experiment, print out the accuracy and then note the best performing model at the end.

Before we get started, let's inspect the dataset a bit first.  Fill in the code right underneath the comments below, and add more cells as needed.
"""

# Do all the imports here
from sklearn.datasets import fetch_lfw_people
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, f1_score, classification_report


import numpy as np
import matplotlib.pyplot as plt

# Load in the dataset
lfw_people = fetch_lfw_people(min_faces_per_person=60) # min faces so the model wont have noise

# How may samples are in the dataset?  How many targets and how many images are there?
# What are the dimensions of each image?  Print out all the results of these questions
# right here.
n_samples, h, w = lfw_people.images.shape
n_features = lfw_people.data.shape[1]
n_classes = len(lfw_people.target_names)

print(f"Number of samples: {n_samples}")
print(f"Number of features: {n_features}")
print(f"Number of classes (unique persons): {n_classes}")
print(f"Dimensions of each image: {h}x{w}")

# Visualize a histogram of the counts for each target label.  For example,
# how many images of target label class 0 do we have?  1?  and so on, for
# all labels.  Are there any observations about the dataset you can note here?
#
# If you can show the target labels with the class names (e.g., the person identities)
# rather than 0, 1, 2, ..., that might look nicer but this is NOT a requirement.

plt.figure(figsize=(10, 4))
plt.hist(lfw_people.target, bins=n_classes)
plt.xlabel('Person')
plt.ylabel('Number of Images')
plt.title('Histogram of Images per Person in LFW Dataset')
plt.xticks(np.arange(n_classes), lfw_people.target_names, rotation=45, ha="right")
plt.tight_layout()
plt.show()

# Observation: the data is imbalanced so I will use stratify when splitting

# Display 3 randomly chosen images using matplotlib.  Consider the colormap so
# that it is pleasing to look at.

fig, ax = plt.subplots(1, 3, figsize=(10, 5))
for i in range(3):
    index = np.random.randint(0, n_samples)
    ax[i].imshow(lfw_people.images[index], cmap='gray')
    ax[i].set_title(lfw_people.target_names[lfw_people.target[index]])
    ax[i].axis('off')
plt.show()

# Pre-process the dataset.  Do any reshaping, normalization, or anything else
# that you need/want to do to get it ready for training and evaluation.  Make sure
# to keep in mind we are training models both with and without normalization.
#
# Remember if you are going to reduce the dimensionality, you must show how/why you
# chose the new parameterization.

# Reshape the data for the models
X = lfw_people.data
y = lfw_people.target

# Split the data into a training set and a test set using stratified sampling
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, stratify=y, random_state=42)

# Prepare scaled versions of the datasets
scaler = StandardScaler().fit(X_train)
X_train_scaled = scaler.transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Function to train and evaluate models
def train_evaluate_model(model, X_train, y_train, X_test, y_test, name):

    model.fit(X_train, y_train)

    if hasattr(model, 'best_params_'):
        best_params = model.best_params_
        print(f"{name}: Best parameters = {best_params}")
    else:
        best_params = None

    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred, average='weighted')

    print(f"{name}:\n Accuracy = {accuracy:.4f}, F1-score = {f1:.4f}")
    return accuracy, f1, best_params, model

# Fill in the remaining parts of the experiment here as you see fit.  At the very
# end you must show which model type worked the best very clearly.  Print out as
# much as you need so that we can "see your work".
#
# Please attempt to write as clean code as possible so that it is both readable
# and well-written.  Code should be written both with the idea that it should
# accomplish the intended goal AND that other people reading it can understand it,
# debug it, and perhaps even take it over one day if need be.
# You may write classes and functions as needed, but do keep the line count to a
# minimum.

# Define models and parameter grids
param_grid_rf = {
    'n_estimators': [10, 150],
    'max_depth': [5, 10],
}
param_grid_svm = {
    'C': [0.1, 1, 10],
    'gamma': ['scale', 'auto'],
}

# Models and their configurations
models_config = [
    ('Logistic Regression', LogisticRegression(max_iter=1300), None, False),
    ('Logistic Regression Normalized', LogisticRegression(max_iter=1300), None, True),
    ('Random Forest', RandomForestClassifier(random_state=42), param_grid_rf, False),
    ('Random Forest Normalized', RandomForestClassifier(random_state=42), param_grid_rf, True),
    ('SVM', SVC(kernel='rbf', random_state=42), param_grid_svm, False),
    ('SVM Normalized', SVC(kernel='rbf', random_state=42), param_grid_svm, True),
]

results = []

for name, model, param_grid, normalize in models_config:
    current_X_train = X_train_scaled if normalize else X_train
    current_X_test = X_test_scaled if normalize else X_test

    if param_grid:
        model = GridSearchCV(model, param_grid, cv=5, scoring='accuracy')

    accuracy, f1, best_params, trained_model = train_evaluate_model(
        model, current_X_train, y_train, current_X_test, y_test, name)
    results.append((name, accuracy, f1, best_params, trained_model))


# Identifying the best model based on accuracy
best_model = max(results, key=lambda x: x[1])
print(f"\nBest model based on accuracy: {best_model[0]} with Accuracy = {best_model[1]:.4f}, F1-score = {best_model[2]:.4f}")

# print classification report for the best model
y_pred_best = best_model[4].predict(X_test_scaled if best_model[0].endswith('Normalized') else X_test)
print(f"\nClassification report for {best_model[0]}:\n{classification_report(y_test, y_pred_best)}")

"""# 2. Repeat the LFW experiment with an MLP using PyTorch (5 points)

Do the same thing we did with scikit-learn above, but now with a neural network.  We will explore a few different things to see what works best:


*   Hidden size: explore with 64 and 128 hidden dims in the MLP layers.
*   Depth: how many layers to the MLP work best?  Try 1, 4, and 8 layers.
*   For each depth, try with and without dropout.  Use a probability of 0.2 for the dropout layers.  When you use dropout, use it after every linear layer except for the last one (which is meant to do the classification).
*   Again, explore both with and without normalization of the input data.  *This yields 24 total experiments.*

In this scenario, we need 3 splits of the dataset: train, validation, and test.  **Again, be sure that the comparisons across the experiments are "fair".**

You must write the training loop yourself as well as the evaluation.  You can use the same code as above to split the data and pre-process it (it's ok to use numpy and sckit-learn functions mixed in with PyTorch code, just be mindful of the data-types; e.g., numpy arrays versus torch.tensors), as well as to evaluate using the classification accuracy metric.

For the training loop, let's use the Adam optimizer with a learning rate of 3e-4, and all other default parameters.  Make sure to use the appropriate loss function, given that this is a multi-class classification problem.  For each given experiment, use a maximum number of epochs (50) and make sure to save the best-performing epoch according to the validation loss so that you can avoid over/under-fitting, **however the result you should report is performance on the left-out test set.**

When you cycle through the hyper-parameter choices, it is up to you how to do this most efficiently.  Please do not make 24 copies of the same training loop in different cells.  Be efficient with your code, and leverage existing tools wherever possible (e.g., scikit-learn, PyTorch tools, write your own functions etc).

**Which hyper-parameter setting worked best? How did this compare with the scikit-learn estimators?**

Again, be mindful of the number of lines of code as well as code cleanliness and readability.  Print out as much necessary info as possible to help "tell the story".

Note: if the deeper models are taking too long to train, feel free to use the GPU runtimes (and note in your code if a GPU is being used.  It is easy to detect if a GPU is present and only use it if it's there, to make your code very re-usable, but this is NOT required.  It is a good exercise though :) )
"""

# in case you dont have it installed
!pip install tqdm

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, TensorDataset
from torch.optim import Adam
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score
import numpy as np
from tqdm import tqdm
import matplotlib.pyplot as plt
import time

# Model Class
class FaceRecognizerMLP(nn.Module):

    def __init__(self,
                 input_size,
                 hidden_size,
                 num_layers,
                 num_classes,
                 use_dropout=False,
                 dropout_prob=0.2):
        super(FaceRecognizerMLP, self).__init__()
        layers = []
        for i in range(num_layers):
            if i == 0:
                layers.append(nn.Linear(input_size, hidden_size))
            else:
                layers.append(nn.Linear(hidden_size, hidden_size))
            if use_dropout and i < num_layers - 1:
                layers.append(nn.Dropout(dropout_prob))
        self.layers = nn.ModuleList(layers)
        self.clf = nn.Linear(hidden_size, num_classes)

    def forward(self, x):
        for layer in self.layers:
            x = F.relu(layer(x))
        x = self.clf(x)
        return x

# Functions


def train_model(model, train_loader, val_loader, device, epochs=50):
    optimizer = Adam(model.parameters(), lr=3e-4)
    criterion = nn.CrossEntropyLoss()
    best_val_loss = float('inf')
    best_model = None
    early_stopping_counter = 0

    train_losses = []
    val_losses = []

    for epoch in range(epochs):
        model.train()
        train_loss = 0
        for inputs, labels in train_loader:
            inputs, labels = inputs.to(device), labels.to(device)
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            train_loss += loss.item()
        train_loss /= len(train_loader)

        val_loss = evaluate_model(model, val_loader, device)

        if val_loss < best_val_loss:
            best_val_loss = val_loss
            best_model = model.state_dict()
            early_stopping_counter = 0  # reset counter if validation loss improves
        else:
            early_stopping_counter += 1

        if early_stopping_counter >= 5:
            print(f"Early stopping triggered at epoch {epoch + 1}")
            break

        train_losses.append(train_loss)
        val_losses.append(val_loss)

    plt.figure(figsize=(10, 5))
    plt.plot(train_losses, label='Training Loss')
    plt.plot(val_losses, label='Validation Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Training and Validation Loss Over Epochs')
    plt.legend()
    plt.show()

    return best_model


def evaluate_model(model, loader, device):
    model.eval()
    criterion = nn.CrossEntropyLoss()
    total_loss = 0
    with torch.no_grad():
        for inputs, labels in loader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            total_loss += loss.item()
    return total_loss / len(loader)


def compute_accuracy(model, loader, device):
    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for inputs, labels in loader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
    return correct / total

# Convert datasets to TensorDatasets and then to DataLoader
# Make sure to preprocess your data: scale if necessary and convert them to tensors
# X_train, X_val, X_test, y_train, y_val, y_test should be defined here

# Example of DataLoader creation
# train_dataset = TensorDataset(torch.Tensor(X_train), torch.LongTensor(y_train))
# train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)

# Split Data
X_train, X_test, y_train, y_test = train_test_split(X,
                                                    y,
                                                    test_size=0.25,
                                                    stratify=y,
                                                    random_state=42)
X_train, X_val, y_train, y_val = train_test_split(X_train,
                                                  y_train,
                                                  test_size=0.1,
                                                  stratify=y_train,
                                                  random_state=42)

# Normalize the data
scaler = StandardScaler().fit(X_train)
X_train_norm = scaler.transform(X_train)
X_val_norm = scaler.transform(X_val)
X_test_norm = scaler.transform(X_test)

# Convert to tensors - Normalized
X_train_norm_tensor = torch.tensor(X_train_norm, dtype=torch.float32)
X_val_norm_tensor = torch.tensor(X_val_norm, dtype=torch.float32)
X_test_norm_tensor = torch.tensor(X_test_norm, dtype=torch.float32)

# Convert to tensors - Not Normalized
X_train_tensor = torch.tensor(X_train, dtype=torch.float32)
X_val_tensor = torch.tensor(X_val, dtype=torch.float32)
X_test_tensor = torch.tensor(X_test, dtype=torch.float32)

y_train_tensor = torch.tensor(y_train, dtype=torch.long)
y_val_tensor = torch.tensor(y_val, dtype=torch.long)
y_test_tensor = torch.tensor(y_test, dtype=torch.long)

# Preparing datasets and loaders for both normalized and non-normalized data
datasets = {
    'normalized': {
        'train': TensorDataset(X_train_norm_tensor, y_train_tensor),
        'val': TensorDataset(X_val_norm_tensor, y_val_tensor),
        'test': TensorDataset(X_test_norm_tensor, y_test_tensor)
    },
    'non_normalized': {
        'train': TensorDataset(X_train_tensor, y_train_tensor),
        'val': TensorDataset(X_val_tensor, y_val_tensor),
        'test': TensorDataset(X_test_tensor, y_test_tensor)
    }
}

loaders = {
    norm: {
        split: DataLoader(dataset=datasets[norm][split],
                          batch_size=64,
                          shuffle=split == 'train')
        for split in ['train', 'val', 'test']
    }
    for norm in ['normalized', 'non_normalized']
}

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
input_size = X_train.shape[1]
num_classes = len(np.unique(y))

hyperparameters = [(hidden_size, num_layers, use_dropout, norm)
                   for hidden_size in [64, 128] for num_layers in [1, 4, 8]
                   for use_dropout in [True, False]
                   for norm in ['normalized', 'non_normalized']]

results = []

for test_num, (hidden_size, num_layers, use_dropout,
               norm) in enumerate(hyperparameters, start=1):
    print(f"\nTest {test_num}:\n")
    start_time = time.time()
    train_loader = loaders[norm]['train']
    val_loader = loaders[norm]['val']
    test_loader = loaders[norm]['test']

    model = FaceRecognizerMLP(input_size, hidden_size, num_layers, num_classes,
                              use_dropout).to(device)
    best_model_state = train_model(model,
                                   train_loader,
                                   val_loader,
                                   device,
                                   epochs=50)
    model.load_state_dict(best_model_state)
    test_accuracy = compute_accuracy(model, test_loader, device)
    end_time = time.time()
    time_taken = end_time - start_time

    # Create a dictionary for the results
    experiment_details = {
        "test_num": test_num,
        "hidden_size": hidden_size,
        "num_layers": num_layers,
        "dropout": use_dropout,
        "normalization": norm,
        "test_accuracy": test_accuracy,
        "time_taken": time_taken
    }

    # Append the details to the results list
    results.append(experiment_details)

    print(f"- Hidden Size: {experiment_details['hidden_size']}\n"
          f"- Layers: {experiment_details['num_layers']}\n"
          f"- Dropout: {experiment_details['dropout']}\n"
          f"- Data: {experiment_details['normalization']}\n"
          f"- Test Accuracy: {experiment_details['test_accuracy']:.4f}\n"
          f"- Time Taken: {experiment_details['time_taken']:.2f} seconds\n")

# Find and print the best model
winning_experiment = max(results, key=lambda x: x['test_accuracy'])
print(
    f"\nBest Model:\n\n"
    f"Hidden Size: {winning_experiment['hidden_size']}\n"
    f"Number of Layers: {winning_experiment['num_layers']}\n"
    f"Dropout: {'Enabled' if winning_experiment['dropout'] else 'Disabled'}\n"
    f"Data Normalization: {winning_experiment['normalization']}\n"
    f"Test Accuracy: {winning_experiment['test_accuracy']:.4f}\n"
    f"Time Taken: {winning_experiment['time_taken']:.2f} seconds")



